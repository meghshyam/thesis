\chapter{Mosaicing Scenes with  Vacant Spaces}
\label{ch:vacantSpaces}
\section{Introduction}
Finding features and using them to align images to construct wide
field of view panoramas is one of the success stories of computer
vision.  Virtually all recent consumer cameras have this technology
embedded.  The success of these methods relies significantly on
finding common features in the images that can be used to establish
the appropriate warps to register the images together.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/vacantSpaces/teaser.pdf}
  \caption[Overview]{ \label{fig:vacantTeaser} The long range photograph of a scene
    taken from an SLR camera is shown in top right.  When such a scene
    is probed by a quadcopter, it results in the input images shown on
    the left (color balance is different from the SLR camera).  The
    state of the art methods (middle column) are unable to make a
    \emph{single} mosaic because the vacant space (third picture on
    the left) does not seem to have any matchable features with
    subsequent input images. Our method handles this situation (bottom
    right).  }
\end{figure}

There are scenes, however, that consist of image content that makes
this challenging.  One situation is when the scene needs to be probed
in an orthographic view and is not easily accessible.  Murals on
urban architecture is an example. Another situation is when scene
patterns and texture are repeated (too many similar features in, e.g.,
an outdoor art exhibition).  This can make it challenging for a
matching algorithm to find appropriate matches in large panoramas.  A
related situation is when a scene area simply does not contain
features (too little, or no features, e.g. posters in an event).
Fig~\ref{fig:vacantTeaser} shows an example of this case.  The state of the
art methods are unsatisfactory.  The idea of a moving quadcopter
taking pictures suggests using a Structure from Motion (SfM) paradigm.
However, based on our experiments with Bundler\cite{Snavely06,
  Snavely07} and VisualSFM \cite{Wu13}, we see that the success of SfM
depends very strongly on ``good'' correspondences between input
images, absent in large vacant (featureless) spaces. Specialized -- state of the art -- image stitching 
methods from \cite{Brown03, Brown05} used in tuned software like Adobe 
Photoshop CS6 or AutoStitch \emph{also} do not work as can be seen 
in Figure~\ref{fig:vacantTeaser}.


The goal of this work is to create panoramic images of scenes using a
quadcopter in situations described above. From a vision perspective,
we are excited about a new mosaicing problem containing large
homogeneous vacant spaces.  This results in scene regions that have no
matches between many significant images, and therefore cannot be
aligned using traditional mosaicing methods.


{\bf Key Idea} We propose to solve the vacant space problem by using
an inexpensive off-the-shelf flying device, such as a quadcopter which
can be assumed to contain an inertial measurement unit (IMU) that has
positional information.  The proximity relationship that the resultant
images have, can be used to significantly reduce the search space in
finding matches.  Further, the proximity relationship also allows, in
principle, to vary the parameters involved in feature selection. For
example, if there is a reason to believe that two images are adjacent
horizontally, one can choose to adjust thresholds in feature matching
algorithm to hunt for otherwise elusive matching pairs.

We note that IMU data can be also made available in other devices such
as smartphones.  An autonomously programmed quadcopter, however, is
particularly enticing because of its ability to fly to areas that are
accessible to the human eye, but inaccessible for the human to
reach. Such areas do not lend themselves easily to high-quality
images. 

Further, IMU data, whether on a smartphone or on a quadcopter, cannot
be relied exclusively, or sometimes at all, especially on inexpensive
devices. Our experiments indicate that the roll and pitch angles
(depending on the distances involved) may be completely off, and so
can the physical coordinates.  This is a consequence of the jerky,
swift movements.  Complementing the IMU with information gleaned from
vision algorithms, however, may be a useful practice.

{\bf Contributions} The main technical contribution of this work is
that it improves the state of the art in mosaicing.  We assume that
the imagery is acquired by a quadcopter for the reasons mentioned
above. Sending a battery of images from a quadcopter to an image
mosaicing algorithm such as AutoStitch incapacitates the algorithm
because of the sheer number of images. Sending a sampled version of
images to a manageable number $N$ of images, with $O(N^2)$ possible
areas to match for features, also does not work since the sampled
image contains vacant space.  In this work, we use IMU information
that lends itself to a graceful $O(N)$ algorithm.  Results are
available in the experiments section. In summary, we have a solution to a new problem, and a
faster solution using IMU data.

%{\bf Limitations} 
In this work, we assume that the scene lies on a planar surface, or
approximately planar surface. The quadcopter can also be programmed to
have a viewing angle perpendicular to any desired planar structure.
The standard homography computation is still not possible because of
the vacant spaces. To overcome this, we reduce the mosaicing problem
to the stereo problem and are thus able to complete the panorama.

% The rest of this paper is organized as follows.  In the next section,
% we discuss related work.  Subsequently, we describe the main steps in
% our process, and justify the process in Section~\ref{sec:results} with
% experimental results. (Additional results are available in
% supplementary material.)  The final section discusses future work.

\section{Methodology}

The goal of this work is to compute a panorama of a scene lying on
a planar surface that has regions of vacant spaces.  A schematic for
this problem is shown in Figure~\ref{fig:schematic}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/vacantSpaces/indoor}\\
  \includegraphics[width=0.8\textwidth]{figures/vacantSpaces/stereoOverlap}\\

  \caption[Problem definition]{ \label{fig:schematic} Problem definition. (Top)
  Vacant spaces are encountered in various scenes.  When individual portions are
    captured by a quadcopter, how does one create the complete mosaic
    given that common features are either not available, or
    confusing? (Bottom) Simplified reduction of the problem to a geometrical structure.
  }
\end{figure}    

The method adopted is pictorially depicted in the overview shown in
Figure~\ref{fig:vacantworkflow} and is described in detail later on.  In
brief, we systematically acquire a video of the scene, reduce the
input video to a manageable number of images, and finally combine the
images acquired from different positions into a mosaic.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{figures/vacantSpaces/Workflow} 
  \caption[Workflow]{ \label{fig:vacantworkflow} Overview: Input imagery is
    systematically acquired (top left) by a quadcopter.  In the next
    step, interesting images are found by clustering the video into
    regions based on positional data.  A graph is constructed using
    proximal images. For each connected component in a graph, standard
    stitching techniques are used to create mini-panoramas which are
    then joined together into the super-panorama 
    again using IMU data.}
\end{figure}    


\subsection{Video acquisition}
We first dispatch the quadcopter to as close to the scene as
possible. The corners of a rectangular area of interest are provided
to the quadcopter, and it is programmed to traverse the area in a
raster scan fashion.  There are various control aspects involved in
sending a quadcopter; in outdoor areas, the quadcopter is impacted by
the wind and it might lose its way.  The control aspects of the quadcopter
are beyond the scope of this work.  \emph{Note that trying to create a
  mosaic in an incremental linear fashion by combining adjacent frames is
  prone to loss of two-dimensional spatial proximal information. It is also
  computationally overwhelming.}

The quadcopter returns with a video of the scene.  Images extracted
from a short video of about a minute or more overwhelms existing
mosaicing software, such as AutoStitch or Adobe Photoshop.  In the
rest of this section, we use AutoStitch to indicate the state of the art
stitching programs such as AutoStitch, Photoshop, etc.

\subsection{Acquiring interesting images}
\label{sec:selection}
Our goal in this step is to reduce the amount of input data and
produce a set of interesting images.  In other words, we wish to
convert a video into an album of images.  The key difference between
our problem and standard albumization \cite{Aner, Lee} is the use of
positional information.  A standard quadcopter has an inertial
measurement unit (IMU) that, after calibration, may give reasonably
accurate information of positions. Using positional information it is
possible to cluster the images and sort the images into an $m\times
n$ grid.  The number of cluster centers is automatically determined
using the agglomerative bottom-up hierarchical clustering method
\cite{Lior}, with the additional requirement that the whole scene
(represented by the positional data) is covered.

{\bf Clustering Details} We assume that each IMU data position
corresponds to an image of definite fixed dimensions.  Consider each
position of the IMU data to be a leaf node. Two nodes are greedily
combined based on the closest Euclidean distance, and replaced with an
internal node; the position of the internal node is set to be the
centroid of the two nodes, and each internal node now corresponds to a
virtual image of the same size taken by a virtual quadcopter.  The
algorithm recursively merges all the nodes till we end up with a root.
In the next phase, we produce cluster centers; a set of nodes is
considered for being the output as cluster centers if the union of
these nodes completely cover the scene. From the bottom-up
construction, it is clear that the root will represent a single
position, and thus a single virtual image, and will not cover the
scene.  At the other extreme, the set of all leaf nodes \emph{will}
cover the scene. To resolve this, during the calibration phase, we
pre-decide the minimum distance between two centers of projection to
have the least overlap. This is used as the threshold in the clustering
algorithm.  Once cluster centers are found, we pick the leaf node
which is closest to the cluster center to find a real image. This
process is schematically shown in Figure~\ref{fig:selection}.

If we had no IMU information, one may consider selecting a
set of interesting images using any appearance-based method such as
optical flow or feature selection.  However, due to the jerky and
uneven motion of the quadcopter, such measures do not prove to be
sufficient. On the other hand, on inexpensive quadcopters, we have
encountered several cases of erroneous positional information (we note
here that image computations are not on-board the quadcopter and IMU
information is transmitted via WiFi to a host computer).  

% We believe
% that combination of standard vision-based clustering method with the
% clustering based on positional information can be used to either drop
% frames, or to correctly include frames based on image content.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.42\textwidth]{figures/vacantSpaces/selection} 
  \includegraphics[width=0.56\textwidth]{figures/vacantSpaces/clustering}
  \caption[Selection of Images]{ \label{fig:selection} Left:We align the image
  stream with IMU data, and then transform the video into a set of interesting
    images with a clustering algorithm. Right: Hierarchical clustering is done
    on the positional data to find out the optimal number of positions which
    `encompasses' the scene. Threshold based on the average distance of the
    quadcopter from the plane is used to prune the tree.}
\end{figure}    

In practice, the number of cluster centers for the scenes we have
covered is now within the capacity of AutoStitch.  As mentioned in the
introduction, as long as there are sufficiently varying and
``matchable'' features, AutoStitch is able to perform a reasonable
result.  However, if there are very few features in the overlapping region
of two images, then the output is not acceptable. This situation will
arise when there is vacant space between two pictures.

{\bf Time complexity} AutoStitch has not been designed
to use positional information. As a result, if there are $N$ input
images, the program has to consider possible matches in approximately
$O(N^2)$ set of areas.  Our program is able to mosaic in an $O(N)$
fashion.

{\bf Mini-Panoramas} Specifically, we assume at this point that the
interesting photos are available in the form of a $m \times n$
grid. First, we find SURF \cite{Bay} features for each image in a
grid. Next, we use Best of Nearest Neighbor matcher (from the OpenCV
library) with Random Sample Consensus (RANSAC) \cite{Fischler1981} to
find geometrically consistent matches between neighborhood images
inside grid.  We create a graph with images being nodes and add an
edge between two nodes if there are sufficient matches. We have to
recall at this point that if there are ``vacant spaces'' there will
not be enough features for successful matches; the graph will end up
with multiple (disconnected) components.  We next compute multiple
spanning trees for the various components. Given a spanning tree, the
center of the spanning tree is a node from which the distance to all
other nodes is minimal \cite{Kocay}. Next, we calculate the homography
of each image with respect to spanning tree center.  Finally, for each
spanning tree, we stitch all pictures within the spanning tree to
create a mini-panorama using the computed homographies by warping all
images with reference to the image at spanning tree center. The
spanning tree is an $O(N)$ structure. The process is described in
Figure~\ref{fig:graph}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\textwidth]{figures/vacantSpaces/graphCreation} 
  \caption[Creation of Mini-panoramas]{ \label{fig:graph} Interesting images
  acquired are segmented and individual (mini-panoramas) are constructed. These
    are then later combined into the desired super-panoramas using IMU data.}
\end{figure}    

\subsection{Super-panorama}
In this section, we consider the situation when programs like
AutoStitch fail.  We assume that the output of the previous step has
resulted in multiple spanning trees where each spanning tree center
corresponds to a specific depth. This is the depth of the center of
the spanning tree (estimated from the IMU data) since we have stitched all images by taking the
spanning tree center as a reference.  Individual panoramas for each
spanning tree termed mini-panoramas have been created. A
super-panorama must be created from mini-panoramas; these usually
correspond to different depths for at least two reasons.

First, it is invariably difficult, if not impossible, to control a
quadcopter to be at the exact depth even in indoor scenes.  The
aerodynamics and the thrust produced tends to make the quadcopter
drift.  Second, it might also be necessary to let the quadcopter probe
and come closer to the scene so as to get a ``good picture''.

A super-panorama is done using a two-step process. Assume two trees in
the forest corresponding to area A and area B of the scene (see
Figure~\ref{fig:stereo}). 
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/vacantSpaces/move} \\
  \includegraphics[width=0.8\textwidth]{figures/vacantSpaces/stereo} 
  \caption[Creation of Super-panoramas]{ \label{fig:stereo} (Top) The virtual
  picture as seen from position B' is computed using Equation~\ref{eq:moveRelation} from the real picture
    taken from B.  (Bottom) Using the stereo disparity, calculated from the baseline 
   width b and depth Z',  it is possible to depict the composite scene obtained from both A
  and B' (from the viewpoint of A).}
\end{figure}    
Assume that a mini-panorama is created from these two areas and the
depth of the planar surface from the camera is more for A, than for
B. We then take the mini-panorama image captured at B, and `move' it to
a new location B' whose depth (from the imaged surface) is the same as
that of A. The resulting image  will be smaller; the images are
related by the equation
\begin{equation}
  \label{eq:moveRelation}
  {\bf \frac{x'}{x} = \frac{Z}{Z'}}
\end{equation}
where {\bf x} (respectively {\bf x'}) represents a pixel location of
the image in B (respectively, B') and 
{\bf Z} (respectively {\bf Z'}) represents the depth of the images
surface from B (respectively, B').

In order to form a super-panorama from the depth of A, we can now
treat the resulting images from A (unchanged) and Bâ€™ (computed from
Equation~\ref{eq:moveRelation}) forming a simplistic stereo pair at
the depth of position A.  Using the stereo disparity formula we can
``place,'' from the viewpoint of A, the image captured from B',
thereby creating a super-panorama. (We could as well present the
entire scene from the viewpoint of B' (since it is at the same depth);
we prefer these pictures to the one that one may be created from the
depth of B.)

% In theory, one may for the super-panorama from viewpoint B, by first
% moving position A to the same depth as of position B. But in that
% case, virtual image of A formed at the depth of position A will be
% ``zoomed-in'' version which may cause pixelization.

\begin{figure}[h]
\includegraphics[width=0.48\textwidth]{images/left}
\includegraphics[width=0.48\textwidth]{images/right}
\caption[Example of super-panorama]{Candidate images for a super-panorama taken
from different depths. For context, see Figure~\ref{fig:results}.  These images are
  `reference' images (spanning  tree center) of individual  mini-panoramas.}
\label{fig:exmaple}
\end{figure}

\textbf{Example:} Consider two images (shown in Figure~\ref{fig:exmaple}) taken from the positions
(1.37, 0.85, 1.6), and (3.75, 0.98, 1.4).  As their depths are
different, we (virtually) move the second image by shrinking the
second image by a factor $\frac{1.4}{1.6}$, i.e., to 87\% of its
original size. With both images at the same depth (1.6m), the 
disparity of the second image is 
\begin{center}
$\text{disparity}_x = (3.75 - 1.37)f/1.6 = 839$\\
$\text{disparity}_y = (0.85 - 0.98)f/1.6 = -45$
\end{center}
where $f$ is focal length of the quadcopter camera in pixel units.

\subsection{Summary: Use of IMU}
The IMU data is used primarily for two purposes:
\begin{enumerate}
\item \textbf{Selection and ordering of images:} We use the IMU data
  to select representative images from the video and arrange them into
  rectangular grid according to the `spatial' neighborhood. It also
  disambiguates situations when multiple images that are spatially
  distant but have similar, repeated features.

\item \textbf{Super-panorama:} Whenever there are no features in
  the overlap region of two images, we use the IMU data to find the
  relative position of one image w.r.t. second image as shown in the
  example. 
\end{enumerate}
\input{experiments_results}

\section{Concluding remarks}

In this chapter, we have described a method of imaging large scenes
using a quadcopter enabling close orthographic views. We also defined
a new problem, that of computing a mosaic of a planar scene with
vacant spaces.  Vacant space relates to images in an input stream
where there are not enough features for traditional mosaicing
algorithms to estimate geometric warps to align the images.

Our solution to this problem is to use a quadcopter which
is capable of taking pictures.  The quadcopter has an inertial
measurement unit that is capable of outputting approximate
positions. Using this positional information, our algorithm selects an
``interesting'' subset of the video imagery.  This subset consists of
pictures taken with a moving camera; we reduce the resulting
problem of computing a mosaic to the stereo problem.  Our
method works on both indoor and outdoor scenes.

Controlling a consumer-focused inexpensive quadcopter can be
problematic; for instance, the quadcopter could have severe yaw and
roll.  We need a robust algorithm for autonomous navigation of quadcopter.
We will discuss vision based algorithms to control such quadcopters autonomously
in next chapter. We extend our work of mosaicing scene on a single planar
surface to scene spread over multiple planar surfaces.
